# AR ASL Prototype
Augmented Reality (AR) for American Sign Language (ASL) Learning

## Description
This is a prototype developed for the project Augmenting Social-Communicative Behaviors Between Hearing Parents and Deaf Children mentored by Dr. Zhen Bai of the University of Rochester.

## Background
Language exposure during infancy is critical for healthy social, emotional, and cognitive development. About 90% of deaf children are born to hearing parents. Deaf children of hearing parents are especially at risk of language deprivation due to the mismatch in communication modalities between the hearing parent, who primarily communicates through spoken language, and the deaf child, who has no access to auditory input. As a result, we see a higher incidence of negative health consequences (e.g. cognitive delays, mental health issues, trauma) in the deaf population. However, we seek to diminish this risk of language deprivation through assistive technology. Our technology aims to tackle the main challenges that prevent hearing parents from providing linguistic input to their deaf children, including a **lack of sign language fluency** and **insufficient communication strategies** that accommodate to the child’s hearing status. Using an augmented reality (AR) lamp model, we hope to enhance parent-child interaction by providing the hearing parent with **awareness of their child’s attention** and with **timely, contextual sign language** to teach their child.

## System Design
AR projection-lamp solution:

## Prototype Usage
Current prototype focuses on projection interface with Wizard of Oz evaluation.
Experimenter acts out both object recognition and eye tracking features.\

Instructions for the experimenter:
1. Mount the projector onto the clamp, and plug the projector into the laptop.
Duplicate the screen.
2. Copy full path of settings.html into browser (preferably Chrome).
3. Search/select up to 3 words for the objects on the table.
4. Then, launch display using the bottom button.
5. Turn the projector on.
6. The screen will be blank in the beginning. Click anywhere to display the labels.
7. Hover over a label to display the corresponding ASL video.


## Support
If there are any questions, feel free to reach out at j.oh@rice.edu.

## Future Work
* Conduct formative interviews and surveys
* Incorporate object recognition and eye-tracking data for automating context-awareness
* Design usability studies for measuring effectiveness
* Explore methods of teaching abstract vocabulary and extended linguistic structures


## Acknowledgements
AR ASL Learning Team:
* Dr. Zhen Bai
* Ashely Tenesaca
* Nina Hu
* Crystal Lee
